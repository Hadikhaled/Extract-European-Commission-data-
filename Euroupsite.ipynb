{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_driver_path = 'C:/Users/Hadi Khaled/Desktop/msedgedriver.exe'\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract links \n",
    "\n",
    "driver = webdriver.Edge(executable_path=r'C:/Users/msedgedriver.exe')\n",
    "\n",
    "# Empty lists to store links\n",
    "categorical_links = []\n",
    "personal_links = []\n",
    "newmain =['//op.europa.eu/en/web/who-is-who/organization/-/organization/SJ/SJ']\n",
    "\n",
    "try:\n",
    "    for item in newmain:\n",
    "        try:\n",
    "            indexs = newmain.index(item)\n",
    "            # Navigate to the URL\n",
    "            driver.get('https:' + item)\n",
    "\n",
    "            # Locate and click the \"Sublevels\" tab\n",
    "            sublevels_tab = driver.find_element_by_id('_eu_europa_publications_portlet_wiw_OrganizationDetailPortlet_SublevelsBtn')\n",
    "            sublevels_tab.click()\n",
    "\n",
    "            # Wait for the page to load completely\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//li[@class=\"tree-node\"]')))\n",
    "\n",
    "            # Get the page source after waiting for the dynamic content to load\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Find categorical links\n",
    "            new_c_links = [a['href'] for a in soup.select('span.tree-label > a')]\n",
    "            if new_c_links:\n",
    "                newmain[indexs + 1:indexs + 1] = new_c_links\n",
    "\n",
    "            # Find personal links\n",
    "            new_p_links = [a['href'] for a in soup.select('.wiw-sublevel-person-name > a')]\n",
    "            if new_p_links:\n",
    "                personal_links.extend(new_p_links)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Print the lists of links\n",
    "print(\"Categorical Links:\")\n",
    "print(categorical_links)\n",
    "print(\"\\nPersonal Links:\")\n",
    "print(personal_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####extract data from links \n",
    "\n",
    "# Create a session object to maintain a persistent connection to the website\n",
    "session = requests.Session()\n",
    "\n",
    "data = []\n",
    "failed_links = []  # List to store links that could not be scraped\n",
    "counter = 0\n",
    "\n",
    "for url in project_links[2050:2177]:\n",
    "    \n",
    "    attempt = 0\n",
    "    success = False\n",
    "    project_data = None\n",
    "    \n",
    "    while attempt < 10 and not success:\n",
    "        try:\n",
    "            # Send a GET request to the website and get the HTML content\n",
    "            URL_HOME = 'https:'\n",
    "            response = session.get(URL_HOME + url, timeout=None)\n",
    "            \n",
    "            while True:\n",
    "                # Check if the response status code is 200\n",
    "                if response.status_code == 200:\n",
    "                    counter += 1\n",
    "                    print(\"Counter:\", counter)\n",
    "                    # Parse the HTML content using the lxml parser\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    break\n",
    "                else:\n",
    "                    response.close()\n",
    "                    response = session.get(URL_HOME + url, timeout=None)\n",
    "                    print(f\"Failed to fetch URL: {url}, Status Code: {response.status_code}\")    \n",
    "                    continue\n",
    "\n",
    "            # Find individual elements within the parent div\n",
    "            name_element = soup.find('h1', id='maincontentgo')\n",
    "            title_element = soup.find('span', class_='wiw-person-detail-current-position')\n",
    "            phone_element = soup.find('div', id='_eu_europa_publications_portlet_wiw_PersonDetailPortlet__phone')\n",
    "            # Find the div with id \"firstPosition\"\n",
    "            first_position_div = soup.find('div', id='firstPosition')\n",
    "\n",
    "            # Check if the div exists\n",
    "            if first_position_div:\n",
    "                # Find all <a> tags within the div\n",
    "                a_tags = first_position_div.find_all('a')\n",
    "                # Extract the text from the last <a> tag\n",
    "                level = a_tags[-1].text.strip() if a_tags else None\n",
    "            else:\n",
    "                level = None\n",
    "\n",
    "            # Extract the text from other elements\n",
    "            name = name_element.text.strip() if name_element else None\n",
    "            title = title_element.text.strip() if title_element else None\n",
    "            phone = phone_element.text.strip() if phone_element else None\n",
    "\n",
    "            # Create a dictionary with the data\n",
    "            project_data = {\n",
    "                'Name': name,\n",
    "                'Title': title,\n",
    "                'Phone': phone,\n",
    "                'Level': level\n",
    "            }\n",
    "            \n",
    "            success = True  # Scraping succeeded\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt == 10:\n",
    "                # If all retry attempts failed, add the URL to the list of failed links\n",
    "                failed_links.append(url)\n",
    "                print(f\"Failed to scrape URL: {url}, Error: {str(e)}\")\n",
    "\n",
    "    # Append the dictionary to the list if scraping was successful\n",
    "    if success:\n",
    "        data.append(project_data)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "output_file = 'E:/Projects/Project.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# Read the existing data in Sheet1 if it exists\n",
    "existing_data = pd.DataFrame()\n",
    "try:\n",
    "    existing_data = pd.read_excel(output_file, sheet_name='Sheet1')\n",
    "except FileNotFoundError:\n",
    "    pass  # No existing data, so leave existing_data as an empty DataFrame\n",
    "\n",
    "# Concatenate the existing data with the new data\n",
    "combined_data = pd.concat([existing_data, df], ignore_index=True)\n",
    "\n",
    "# Write the combined data to the Excel file starting from the last row of existing data\n",
    "with pd.ExcelWriter(output_file, mode='w') as writer:\n",
    "    combined_data.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "# Print the links that could not be scraped\n",
    "print(\"Failed links:\")\n",
    "for link in failed_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract emails\n",
    "\n",
    "# Initialize Edge webdriver\n",
    "driver = webdriver.Edge(executable_path=r'C:/Users/msedgedriver.exe')\n",
    "\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    for item in project_links[101:500]:\n",
    "        try:\n",
    "            # Navigate to the URL\n",
    "            driver.get('https:' + item)\n",
    "\n",
    "            span_element = WebDriverWait(driver, 1000).until(EC.element_to_be_clickable((By.CLASS_NAME, 'address-email')))\n",
    "\n",
    "            # Click on the span element\n",
    "            span_element.click()\n",
    "\n",
    "            # Wait for the page to load completely\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//span[@class=\"address-email\"]')))\n",
    "\n",
    "            # Get the page source after waiting for the dynamic content to load\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Find the anchor tag inside the span element\n",
    "            email_link = soup.find('span', class_='address-email').find('a')\n",
    "\n",
    "            # Extract the email address and the mailto link\n",
    "            email_address = email_link.text\n",
    "            \n",
    "            # Create a dictionary with the data\n",
    "            project_data = {\n",
    "                'email_address': email_address\n",
    "            }\n",
    "\n",
    "            print(\"Email address:\", email_address)\n",
    "\n",
    "            data.append(project_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Specify the path for the Excel file\n",
    "email_file = 'E:/Projects/email.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if not os.path.exists(email_file):\n",
    "    # If the file doesn't exist, write the DataFrame to a new Excel file\n",
    "    with pd.ExcelWriter(email_file, mode='w') as writer:\n",
    "        pd.DataFrame(data).to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "else:\n",
    "    # Read the existing data in Sheet1 if it exists\n",
    "    existing_data = pd.DataFrame()\n",
    "    try:\n",
    "        existing_data = pd.read_excel(email_file, sheet_name='Sheet1')\n",
    "    except FileNotFoundError:\n",
    "        pass  # No existing data, so leave existing_data as an empty DataFrame\n",
    "\n",
    "    # Concatenate the existing data with the new data\n",
    "    combined_data = pd.concat([existing_data, pd.DataFrame(data)], ignore_index=True)\n",
    "\n",
    "    # Write the combined data to the Excel file starting from the last row of existing data\n",
    "    with pd.ExcelWriter(email_file, mode='w') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='Sheet1', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
